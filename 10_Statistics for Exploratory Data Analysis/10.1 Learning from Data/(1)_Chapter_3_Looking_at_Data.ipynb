{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Chapter Three: Inductive Inference, Normal Distribution, Probability Distributions, Z-scores, and Populations (page 73-94)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inductive Reasoning\n",
    "\n",
    "Summary: Inductive inference is a method of reasoning that generalizes from specific observations or instances to broader patterns or principles. In statistics, this means that we analyze a sample of data and make inferences about the entire population based on our findings. Since inductive reasoning doesn't guarantee true conclusions, it's crucial to be aware of uncertainties and make informed decisions based on the available evidence.\n",
    "\n",
    "\n",
    "Analogy: Imagine you're an ornithologist studying a particular species of birds. You've observed a group of these birds in a specific location and noticed that they predominantly have red feathers. Based on this observation, you might make an inductive inference that the majority of the birds in this species have red feathers. This conclusion is not guaranteed to be true, as you have only observed a limited sample of the entire population, but it gives you an initial understanding to guide further research or make predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normal Distributions\n",
    "A normal distribution, also known as a Gaussian distribution or bell curve, is a continuous probability distribution characterized by its symmetric bell-shaped curve. Most of the data points are clustered around the mean, and the distribution is described by two parameters, mean (μ) and standard deviation (σ). The normal distribution is widely used in statistics, as it often occurs naturally and has convenient mathematical properties.\n",
    "\n",
    "Picture a classroom of students who just received their scores on a test. If you plot the distribution of test scores, you may notice that most students scored around the average, with fewer students achieving very high or very low scores. The shape of this distribution could resemble a bell curve, with the peak representing the mean (average) score and the curve's width relating to the standard deviation (the spread of the scores). In this case, the test scores follow a normal distribution, which helps us understand the students' performance patterns and make inferences about their abilities."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Population and Probability Distributions\n",
    "\n",
    "A population is a complete set of items, individuals, or events that share a common characteristic, while a sample is a subset of the population. Probability distribution is a mathematical function that describes the likelihood of different outcomes in an experiment or random process. It can be discrete (e.g., a coin flip) or continuous (e.g., normal distribution). Understanding the distribution of the population helps in making inferences about the population parameters from the sample data.\n",
    "\n",
    "Imagine you are at a fruit market, and you want to understand the distribution of apple sizes. The apples in the market represent the population.\n",
    "\n",
    "In this case, the population distribution would describe how apple sizes are spread across the entire market. For instance, you might find that most apples are medium-sized, with a smaller number of large and small apples. The distribution could be visualized as a histogram or curve that shows the number of apples in each size category.\n",
    "\n",
    "If the distribution of apple sizes is approximately symmetric and bell-shaped, then it can be considered a normal distribution. Understanding the nature of the population distribution helps in making inferences about various characteristics, such as the average size, range, or proportion of apples within specific size categories.\n",
    "\n",
    "Keep in mind that it is often impractical to measure every single apple in the market, so instead, you might take a sample of apples (a subset of the population) and use that to make inferences about the entire population's distribution. Different sampling methods can be employed to ensure that the sample is representative of the population and leads to accurate conclusions.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Z-Scores\n",
    "\n",
    "A Z-score, or standard score, is a measure that indicates how many standard deviations an observation or data point is from the mean of a distribution. It is calculated as (X - μ) / σ, where X is the data point, μ is the mean, and σ is the standard deviation. Z-scores are useful for comparing data points from different distributions or units, as well as identifying outliers and determining percentiles.\n",
    "\n",
    "Imagine you're a coach of a basketball team and have players of various heights. You want to compare the heights of your players to determine how tall or short they are compared to the team's average height.\n",
    "\n",
    "To make this comparison, you calculate the average height and standard deviation of your team's heights. Next, you calculate the Z-score for each player. The Z-score tells you how many standard deviations a player's height is away from the team's average height.\n",
    "\n",
    "For example, let's say the average height of your team is 6 feet, and the standard deviation is 4 inches. A player with a height of 6 feet 4 inches would have a Z-score of 1, as they are one standard deviation taller than the average. On the other hand, a player who is 5 feet 8 inches tall would have a Z-score of -1, meaning they are one standard deviation shorter than the average.\n",
    "\n",
    "In this analogy, Z-scores help you understand each player's height relative to the team's average height, allowing for easier comparison and identification of taller or shorter players. Z-scores can also be applied to various other contexts, such as test scores, income levels, or any other variable where you need to compare values relative to a mean and standard deviation.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nature of Populations\n",
    "\n",
    "The nature of populations can vary significantly, and understanding their properties is essential in making accurate inferences. Key factors to consider include size, homogeneity, shape of the distribution, and the presence of outliers. The chapter reminds readers that errors can occur at various stages of statistical analysis, such as data collection, sampling, and interpretation. To minimize these errors, it is crucial to develop sensible judgments about your data and exercise caution in your analysis.\n",
    "\n",
    "Imagine you're a marine biologist studying different fish species in a large lake. Each fish species represents a distinct population, and understanding the nature of these populations is essential for your research.\n",
    "\n",
    "Size: The number of fish in each species can vary significantly. Some species may be abundant, while others could be rare. The size of the population will impact the sampling methods you employ and the confidence you can have in your inferences.\n",
    "\n",
    "Homogeneity: The fish species might differ in terms of how homogeneous their members are. Some species might exhibit consistent characteristics, such as similar colors, sizes, or behaviors, while others might show considerable variation among individual fish. Understanding the degree of homogeneity within a population will help you determine the appropriate statistical methods to analyze the data.\n",
    "\n",
    "Distribution Shape: The distribution of various attributes, like fish sizes within a species, might take different shapes. Some species might have a normal distribution with most fish having an average size, while others might have a skewed distribution with many small fish and a few large ones. Recognizing the distribution shape is crucial for choosing the right statistical techniques for analysis.\n",
    "\n",
    "Presence of Outliers: Some species might have occasional outliers or unusual cases that significantly deviate from the norm, such as a fish with an unusually long tail. These outliers can impact your statistical analysis, and it's important to identify and account for them when making inferences about the population.\n",
    "\n",
    "By understanding the nature of these fish populations, you can choose appropriate sampling techniques, make more accurate inferences, and better inform your research and conservation efforts. Similarly, in statistics, understanding the nature of populations helps researchers make more reliable conclusions from the data they gather."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "The Central Limit Theorem is a fundamental result in probability theory and statistics. It states that, given a sufficiently large sample size, the sampling distribution of the sample means will approach a normal distribution, regardless of the shape of the population distribution. This powerful theorem allows for the use of statistical techniques based on normal distribution even when the population distribution is not normal.\n",
    "\n",
    "Imagine you have a large jar of differently shaped and colored candies. The candies in the jar represent the population, and each candy's shape and color represent the unique characteristics of the population members. The distribution of these characteristics within the jar might not follow a normal distribution; for example, there could be a higher proportion of star-shaped candies.\n",
    "\n",
    "Now, you decide to take small handfuls of candies from the jar (representing samples) multiple times. Each time you take a handful, you count the proportion of star-shaped candies in that particular sample. Although the individual samples might have varying proportions of star-shaped candies, the Central Limit Theorem (CLT) suggests that as you continue to take more samples, the distribution of the average proportions of star-shaped candies from all the samples will approach a normal distribution.\n",
    "\n",
    "The key takeaway of the CLT is that regardless of the original distribution of the population (the candies in the jar), when you take a large number of samples, the distribution of the sample means (average proportions of star-shaped candies in the samples) will approach a normal distribution.\n",
    "\n",
    "This powerful theorem has significant implications for data analysis, as it enables the use of statistical techniques based on the normal distribution, even if the population distribution is not normal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample Methods\n",
    "\n",
    "Different sampling methods can be employed to select a representative subset of the population for analysis. Some common methods include simple random sampling, stratified sampling, cluster sampling, and systematic sampling. Each method has its pros and cons, and the choice of the sampling method depends on the specific research question and population characteristics.\n",
    "\n",
    "Imagine you are an event organizer conducting a survey to gauge audience satisfaction at a large music festival. There are various stages, each with different genres of music being performed. The festival attendees are the population you want to survey. Due to the event's size and time constraints, it's impossible to survey every attendee. So, you decide to use different sampling methods to select a representative subset of the festivalgoers.\n",
    "\n",
    "1. Simple Random Sampling:\n",
    "You randomly pick attendees throughout the festival, ensuring each person has an equal chance of being selected. This method is unbiased and straightforward but may not account for the diversity of the attendees in terms of their genre preferences or other characteristics.\n",
    "2. Stratified Sampling:\n",
    "You divide the festival attendees into groups based on the stage (genre) they're watching. From each group, you randomly select a proportional number of attendees to survey. This method ensures that fans of each music genre are represented in the survey, which may result in more accurate satisfaction estimates.\n",
    "3. Cluster Sampling:\n",
    "You divide the festival into geographical sections, like seating areas or food vendor zones, and randomly select a few of these clusters. Then, you survey all attendees present in the selected clusters. This method can save time and effort but may introduce more sampling errors compared to other methods, especially if there's significant variability within clusters.\n",
    "4. Systematic Sampling:\n",
    "You select every nth person entering the festival or a specific stage. This method is more efficient than simple random sampling but may introduce bias if there is a systematic pattern among attendees (e.g., fans of a specific genre arriving at certain times).\n",
    "\n",
    "In this analogy, each sampling method has its pros and cons, and the choice depends on factors like the research objective, population characteristics, and resource constraints. The goal is to select a sample that accurately represents the entire population, ensuring that the survey results provide reliable insights into the overall satisfaction of festival attendees."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Estimation of Confidence Intervals\n",
    "\n",
    "Estimation is the process of using sample data to make inferences about population parameters, such as the mean or proportion. Point estimates are single values that best represent the parameter, whereas interval estimates, or confidence intervals, provide a range within which the true population parameter is likely to lie. Confidence intervals take into account the sampling variability and allow for the expression of the level of uncertainty associated with the estimate.\n",
    "\n",
    "Imagine you work at a large shoe store, and you want to estimate the average shoe size of the customers. Since it's impractical to measure every customer's shoe size, you decide to take a random sample of customers who visit the store and record their shoe sizes.\n",
    "\n",
    "After collecting the data, you calculate the sample mean (average shoe size) and the sample standard deviation. To estimate the true average shoe size of all customers visiting the store, you construct a confidence interval using this sample information.\n",
    "\n",
    "For example, let's say you calculate a 95% confidence interval for the average shoe size to be between 7 and 9. This means that you are 95% confident that the true average shoe size of all customers visiting the store falls within this range (7 to 9).\n",
    "\n",
    "The confidence interval accounts for the variability in the sample and provides a range within which the true population parameter (the average shoe size of all customers) is likely to lie. The wider the confidence interval, the higher the uncertainty in your estimate. The level of confidence (e.g., 95%) represents the probability that the true average shoe size falls within the calculated interval, based on the sample data you have.\n",
    "\n",
    "In summary, confidence intervals help quantify the uncertainty associated with your estimate, making it more informative and reliable when drawing conclusions about the entire population (in this case, the average shoe size of all customers visiting the store)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "Hypothesis testing is a statistical technique used to determine if there is enough evidence to reject a null hypothesis in favor of an alternative hypothesis. It involves formulating null and alternative hypotheses, selecting a significance level, calculating test statistics, and comparing them with critical values or p-values to determine the outcome of the test. This method is widely used in various fields to test the validity of claims and make informed decisions.\n",
    "\n",
    "A farmer wants to know if a new fertilizer can increase crop yield compared to their current fertilizer. To do this, they decide to conduct a field experiment.\n",
    "\n",
    "Before starting the experiment, the farmer establishes the null hypothesis (H0) and alternative hypothesis (H1):\n",
    "\n",
    "H0: The new fertilizer has no significant effect on crop yield compared to the current fertilizer.\n",
    "H1: The new fertilizer has a significant effect on crop yield compared to the current fertilizer.\n",
    "\n",
    "The farmer divides their field into two sections, ensuring that the soil quality and sunlight exposure are similar in both sections. Section A is treated with the current fertilizer, and Section B is treated with the new fertilizer. After a growing season, the farmer measures the crop yield from both sections.\n",
    "\n",
    "The farmer then analyzes the results using statistical methods and calculates a p-value. The p-value represents the probability of obtaining the observed results (or more extreme) if the null hypothesis is true. For example, let's say the farmer calculates a p-value of 0.04.\n",
    "\n",
    "In hypothesis testing, the p-value is compared to a predetermined significance level, often denoted as alpha (α). A common alpha value is 0.05 (5%). If the p-value is less than or equal to alpha, the null hypothesis is rejected in favor of the alternative hypothesis. In this case, since the p-value (0.04) is less than alpha (0.05), the farmer rejects the null hypothesis and concludes that there is a significant effect of the new fertilizer on crop yield compared to the current fertilizer.\n",
    "\n",
    "In this analogy, hypothesis testing allows the farmer to make data-driven decisions based on the evidence collected from the field experiment. By comparing the results to a predefined threshold (alpha), the farmer can determine whether the observed effects are statistically significant or just due to chance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Take away Notes\n",
    "\n",
    "The chapter introduces basic statistical concepts by applying them in a basic inductive inference process. Inductive inference = the process of learning about general principles from specific examples, and involves four stages and three transitions (signaled by the ‘→’ below):\n",
    "\n",
    "Data → 2. Sample → 3. Study Population → 4. Target Population Our Data is our raw data: whichever dataset we have to work with.\n",
    "\n",
    "Our Sample is the set of things our Data gives us information about. The transition from our Data to our Sample requires that our Data give accurate information about our Sample.\n",
    "\n",
    "Our Study Population is the group we are actually studying, that is, the set of things that could have been in our Sample. Perhaps our Sample is just the Study Population: this occurs when we have all the data. But most of the time, things are in our Study Population that aren’t in our Sample; most of the time, there are things that could have been in our Sample, that aren’t. The transition from Sample to Study Population in the inductive inference process requires that our Sample is representative of the Study Population.\n",
    "\n",
    "Our Target Population is the set of things we want to draw conclusions on (or discover things about). The transition from the Study Population to the Target Population again requires that the Study Population is representative of the Target Population.\n",
    "\n",
    " A population can be thought of as a physical group of individuals, but also as providing the probability distribution for a random observation: the probability of a random observation having a certain value for some attribute of interest.\n",
    "\n",
    "The probability distribution is the pattern in the whole group of interest.\n",
    "The normal distribution is correctly visualized with a bell-shaped curve, and can be mathematically characterized in terms of its mean and standard deviation: roughly 95% of a normally distributed population will be contained in the interval given by the mean +- two standard deviations, and 99.8% of the data in the central +- three standard deviations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CHAPTER 4: What Causes What?\n",
    "\n",
    "Please read Chapter Four of AoS, pages 95-119. You’ll look at the distinction between correlation and causation, see how good experimental design will help you conclude causation from a set of observations, and learn about Simpson’s Paradox, one of the most important concepts in theoretical statistics.\n",
    "\n",
    "The reading will cover the following concepts:\n",
    "\n",
    "Correlation and Causation\n",
    "Randomized Controlled Trials (RCTs)\n",
    "A/B Testing\n",
    "Prospective, Retrospective, and Case-Control Studies\n",
    "Confounding Variables\n",
    "Adjustment\n",
    "Simpson’s Paradox\n",
    "Concluding Causation from Observational Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chapter 4 Summary\n",
    "\n",
    "Chapter Four of \"The Art of Statistics\" dives into the critical distinction between correlation and causation, and the importance of experimental design in drawing conclusions from observations. The chapter also introduces Simpson's Paradox, a significant concept in theoretical statistics. The key concepts covered in this chapter are:\n",
    "\n",
    "Correlation and Causation: The chapter emphasizes the crucial difference between correlation (a statistical relationship between two variables) and causation (one variable directly causing a change in another). A strong correlation does not always imply causation, as other factors, such as confounding variables, may be involved.\n",
    "\n",
    "Randomized Controlled Trials (RCTs): RCTs are a gold standard in experimental design, used to establish causation between an intervention and an outcome. Participants are randomly assigned to treatment and control groups, minimizing bias and allowing researchers to confidently infer causality if the results show a significant difference between the groups.\n",
    "\n",
    "A/B Testing: A/B testing is a practical application of RCTs, often used in marketing and product development. Two versions of a product or web page (A and B) are compared to determine which performs better in terms of specific metrics (e.g., click-through rates, conversions). Random assignment of users to the versions ensures fair comparison and helps establish causal relationships between design changes and outcomes.\n",
    "\n",
    "Prospective, Retrospective, and Case-Control Studies: The chapter introduces three types of observational studies used when RCTs are not feasible or ethical. Prospective studies follow participants over time, collecting data on exposures and outcomes. Retrospective studies examine past data to uncover relationships between exposures and outcomes. Case-control studies compare individuals with a specific condition (cases) to those without it (controls), investigating possible exposure differences between the groups.\n",
    "\n",
    "Confounding Variables: Confounding variables are external factors that can influence the relationship between two variables, potentially creating a misleading correlation. Understanding and controlling for confounding variables is crucial in determining causal relationships.\n",
    "\n",
    "Adjustment: The chapter discusses the process of adjustment, which involves using statistical techniques to account for confounding variables and more accurately estimate the causal effect between two variables.\n",
    "Simpson's Paradox: This paradox occurs when a trend observed within separate groups reverses when the groups are combined. The chapter demonstrates how Simpson's Paradox highlights the importance of considering confounding variables and properly segmenting data to avoid drawing incorrect conclusions.\n",
    "\n",
    "Concluding Causation from Observational Data: The chapter concludes by discussing the challenges in inferring causation from observational data and the importance of considering factors like the strength of association, consistency, specificity, temporality, biological gradient, plausibility, coherence, and experimental evidence to build a stronger case for causation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chapter 4 Take Away Notes\n",
    "\n",
    "The chapter introduces one of the most fascinating challenges in statistics: inferring causation from our observations.\n",
    "\n",
    "The statistical concept of causation is as follows: a type of event X causes a type of event Y just if Y-events happen more often when X-events occur than when X-events don’t occur.\n",
    "● e.g: Smoking causes cancer because cancer-events happen more often when smoking-events happen than when smoking-events don’t happen.\n",
    "\n",
    "Two consequences of this concept are:\n",
    "1. We can infer causation with confidence only by performing experiments\n",
    "2. The more we intervene, the more evidence for causation we accumulate.\n",
    "Clinical trials try to establish causation. Proper clinical trials should be:\n",
    "- Controlled: i.e., have an intervention group (= a group given some exposure of interest),\n",
    "and a control group (= a group not given that exposure).\n",
    "- Allocated properly: i.e., the intervention and control groups are made as similar\n",
    "(excepting the exposure of interest) as possible by random selection.\n",
    "- Allocated rigidly: i.e., once an individual is assigned to a group, this doesn’t change\n",
    "- Blinded:\n",
    "- Single-blinding means the individuals in the groups don’t know what groups they’re in\n",
    "\n",
    " - Double-blinding means the people monitoring those individuals don’t know this either\n",
    "- Triple-blinding means the statisticians don’t know this, too.\n",
    "- Equal treatment: i.e., the groups are treated as similarly as possible (excepting the\n",
    "exposure of interest).\n",
    "- Everyone is measured: i.e., effects of exposure (or not) should be followed up\n",
    "- Multiple studies: i.e., more than one study should be carried out.\n",
    "- Systematic review: i.e., if more than one study has been done, collate all the evidence.\n",
    "\n",
    "\n",
    "A variable is a confounder exactly if it’s associated with both a response variable and a predictor variable, and may explain some of the relationships between them.\n",
    "● e.g: Age is a confounder with respect to the response variable of weight and the predictor variable of height.\n",
    "The simplest method for handling confounders is to look at the apparent relationship within each level of the confounder. This is known as adjustment/stratification.\n",
    "Simpson’s paradox occurs when an apparent relationship reverses its sign when a confounding variable is taken into account."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation and Causation\n",
    "Imagine that during the summer months, you notice that both ice cream sales and the number of sunburn cases increase. You find a strong correlation between ice cream sales and sunburns. However, this does not mean that eating ice cream causes sunburns or that sunburns cause people to eat more ice cream.\n",
    "\n",
    "In this scenario, there is a third variable, which is the hot, sunny weather. When the weather is hot and sunny, more people tend to spend time outdoors, increasing their risk of getting sunburned. Simultaneously, hot weather also leads to higher ice cream sales, as people look for ways to cool off.\n",
    "\n",
    "The hot weather acts as a confounding variable, creating a correlation between ice cream sales and sunburns. In reality, the two variables are not directly causally linked. The hot weather is the actual cause behind the increase in both ice cream sales and sunburns.\n",
    "\n",
    "This analogy demonstrates that while correlation can help identify relationships between variables, it doesn't necessarily mean there is a causal relationship between them. It's essential to examine the context and any potential confounding variables before concluding causation from a correlation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Randomized Controlled Trials\n",
    "\n",
    "Imagine you're a gardener who wants to test the effectiveness of a new fertilizer in promoting plant growth. To do this, you decide to conduct a gardening experiment with two groups of plants.\n",
    "\n",
    "In a Randomized Controlled Trial (RCT), you would randomly assign the plants to two groups: a treatment group and a control group. The treatment group receives the new fertilizer, while the control group receives no fertilizer or a standard one. Random assignment helps ensure that any differences between the groups, such as soil quality, sunlight exposure, or other factors, are distributed equally and do not bias the results.\n",
    "\n",
    "After a certain period, you measure the growth of plants in both groups. If you find a significant difference in growth between the treatment and control groups, with the treatment group showing increased growth, you can confidently conclude that the new fertilizer has a causal effect on plant growth.\n",
    "\n",
    "The strength of an RCT lies in its random assignment of subjects to the groups, which helps control for potential confounding variables and allows for a more accurate assessment of causality. In this gardening analogy, the RCT design allows you to isolate the effect of the new fertilizer and make a well-supported conclusion about its effectiveness in promoting plant growth."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A/B Testing\n",
    "\n",
    "Imagine you're a restaurant owner who wants to increase dessert sales. You suspect that changing the presentation of the dessert menu might influence customers' decisions. To test this hypothesis, you decide to conduct an A/B test.\n",
    "\n",
    "In the A/B test, you create two versions of the dessert menu:\n",
    "\n",
    "A: The original menu, with text descriptions of each dessert.\n",
    "B: A new menu featuring a photo of each dessert next to the text description.\n",
    "\n",
    "To conduct the A/B test, you randomly give half of your customers the original menu (A) and the other half the new menu (B) over a period of time. You then track dessert sales for both groups.\n",
    "\n",
    "After analyzing the data, you find that customers who received Menu B (with photos) ordered desserts more frequently than those who received Menu A (text descriptions only). Based on this A/B test, you can confidently make a data-driven decision to switch to the new menu design (Menu B) to increase dessert sales.\n",
    "\n",
    "This analogy illustrates how A/B testing works by comparing two different versions of a product or service (in this case, the dessert menu) and measuring their performance against a specific metric (dessert sales). Random assignment of customers to the different versions helps ensure a fair comparison and allows for causal conclusions to be drawn about the effect of the change on the outcome."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
